---
layout: default
title: "Neural Long-Term Memory for Enhanced Sequence Modeling"
date: 2025-06-11
---

<p align="center">
    <img src="https://github.com/fermaat/fermaat.github.io/blob/main/files/images/Titans/Titans0.png?raw=true" alt="Awesome T, as pictured in the paper" width="60">
</p>


Thereâ€™s been a lot of noise lately about scaling, trillion-parameter models, and ultra-long context lengths. But while most attention went to the usual suspects, one paper from the past few months really stood out to me â€” and maybe didnâ€™t get the buzz it deserved:

ğŸ“„ **Titans: Neural Long-Term Memory for Enhanced Sequence Modeling**  
ğŸ”— [Link to paper](https://buff.ly/3WPXG1Q), Ali Behrouzâ€ , Peilin Zhongâ€ , and Vahab Mirrokniâ€ .

I think itâ€™s a pretty interesting one. It basically brings an innovation to how models manage memory and context â€” something thatâ€™s getting more and more relevant these days.

---

### ğŸ¤¯ Memory and Test Time

We already have LLMs that handle *massive* contexts (Gemini's around 2M tokens, if I'm not wrong). Thatâ€™s cool, but letâ€™s be honest â€” **longer doesnâ€™t always mean better**. Sometimes, longer contexts just mean less attention to detail.

**Titans propose some kind of long-term memory** (yeah, LSTMs, I love seeing you again!) that **learns at test time**.

Let me repeat that: **at test time!**

The model dynamically identifies parts of the context and flags them as relevant or not â€” so it knows what to remember. It does this through a clever metric the authors defined: **â€œSurpriseâ€** â€” which basically measures how much a piece of context changes over time. The more it surprises the model, the more attention it gets. 

<p align="center">
    <img src="https://github.com/fermaat/fermaat.github.io/blob/main/files/images/Titans/Titans4.png?raw=true" alt="Surprise! as defined in the paper" width="300">
</p>


---

### ğŸ§  Different Memory strategies

The authors propose three different approaches for using this memory module:

- **MAC**: Memory as context (add it to the input)
- **MAG**: Memory as a gate (like an LSTM-ish twist!)
- **MAL**: Memory as a layer (plug it in the architecture)

I personally liked **MAG** a lot â€” it gave me LSTM flashbacks in a good way. Instead of the usual gating, they use **sliding window attention** moving through time with memory outputs, which makes it much more adaptive. Feels like a neat evolution of the old tricks.

<p align="center">
    <img src="https://github.com/fermaat/fermaat.github.io/blob/main/files/images/Titans/Titans2.png?raw=true" alt="MAG architecture, as defined in the paper">
</p>

---

### ğŸ“Š The results? Titans-level.

The results are quite impressive, but my favorite part was the **â€œneedle in a haystackâ€** test â€” you know, hiding info in a long context and seeing if the model can find it to answer questions.

Guess who was on top?  
**Of course, the Titans.**

---

### ğŸš€ Why Iâ€™m excited

This paper brought back some memories (pun intended) of earlier architectures but added a modern spin that I found super refreshing. Iâ€™m really looking forward to experimenting with this one and seeing what kind of impact it can have in real-world tasks where remembering matters.

Maybe itâ€™s time memory made a real comeback.

